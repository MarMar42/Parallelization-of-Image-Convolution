{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4udel5CpdHn",
        "colab_type": "code",
        "outputId": "a47e71a1-861d-4f98-98e6-31ac2bf71a31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        }
      },
      "source": [
        "!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
        "%load_ext nvcc_plugin"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning git://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-8gyqhp1n\n",
            "  Running command git clone -q git://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-8gyqhp1n\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-cp36-none-any.whl size=4307 sha256=effe2b6a264b464e35c78fe9ca7be657e8ef952cdc0377584362be5329b63cb4\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bdkyx24o/wheels/10/c2/05/ca241da37bff77d60d31a9174f988109c61ba989e4d4650516\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n",
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_qjdTSfplXn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include \"/content/drive/My Drive/Colab Notebooks/HPC-Assignment-3/Helper/inc/helper_cuda.h\" \n",
        "#include \"/content/drive/My Drive/Colab Notebooks/HPC-Assignment-3/Helper/inc/helper_functions.h\"\n",
        "\n",
        "#define Iter 10 //Number of iterations\n",
        "\n",
        "#define N 3 //Size of the mask\n",
        "\n",
        "#define str \"harvard1080.pgm\"\n",
        "\n",
        "#define tile_size 16\n",
        "\n",
        "#define block_size (tile_size + N -1)\n",
        "\n",
        "//Filters\n",
        "float identity[9] = {0,0,0,0,1,0,0,0,0};\n",
        "float edge[9] = {-1,0,1,-2,0,2,-1,0,1};\n",
        "float sharp[9] = {0,-1,0,-1,5,-1,0,-1,0};\n",
        "\n",
        "float blur9[9] = {(float) 1/9,(float) 1/9,(float) 1/9,(float) 1/9,(float) 1/9,(float) 1/9,(float) 1/9,(float) 1/9,(float) 1/9};\n",
        "float blur25[25] = {(float) 1/25,(float) 1/25,(float) 1/25,(float) 1/25,(float) 1/25,(float) \n",
        "                    1/25,(float) 1/25,(float) 1/25,(float) 1/25,(float) 1/25,(float) 1/25,(float) 1/25,(float) \n",
        "                    1/25,(float) 1/25,(float) 1/25,(float) 1/25,(float) 1/25,(float) 1/25,(float) 1/25,(float) \n",
        "                    1/25,(float) 1/25,(float) 1/25,(float) 1/25,(float) 1/25,(float) 1/25};\n",
        "\n",
        "float* filter = blur9;\n",
        "char* filter_s = \"Blur 3x3\";\n",
        "\n",
        "\n",
        "//Constant Memory Allocation\n",
        "__constant__ float deviceFilter[N * N];\n",
        "\n",
        "// Texture reference for 2D float texture\n",
        "texture<float, 2, cudaReadModeElementType> tex;\n",
        "\n",
        "\n",
        "void printArray(float* arr, int width,int height)\n",
        "{\n",
        "    for (int i=0;i<width*height;i++)\n",
        "    {\n",
        "        int row = i/height;\n",
        "        int col = i % width;\n",
        "        if (col == 0)\n",
        "        {\n",
        "            printf(\"\\n\");\n",
        "        }\n",
        "    printf(\"%f \",arr[row*height + col]);\n",
        "\n",
        "    }\n",
        "}\n",
        "\n",
        "void checkCUDAError(const char *msg) \n",
        "{\n",
        "    cudaError_t err = cudaGetLastError();\n",
        "    if( cudaSuccess != err) {\n",
        "        fprintf(stderr, \"Cuda error: %s: %s.\\n\", msg, cudaGetErrorString( err) );\n",
        "        exit(EXIT_FAILURE);\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "//function for padding the input image with zeros for the convolution/////\n",
        " void padding(float* src, int width,int height,int pad_size,float* out,int padded_w,int padded_h){\n",
        "            for (int i = 0; i< padded_h;++i){\n",
        "                for(int j = 0;j<padded_w;++j){\n",
        "                  if(i < pad_size || j < pad_size || i >= (height+pad_size) || j >= (width+pad_size)){\n",
        "                    out[i*padded_w + j]=0.0;\n",
        "                }\n",
        "                else{\n",
        "                    out[i*padded_w + j] = src[(i-pad_size)*width + (j-pad_size)];\n",
        "\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "}\n",
        "\n",
        "//Serial Convolution\n",
        "void Seq_convolution(float* src,float* Filter,int filter_dim, int width, int height, float* output,\n",
        "                     int padder)\n",
        "{\n",
        "    float Sum;\n",
        "    float val,fval;\n",
        "    for (int i = padder; i< height-padder;i++)\n",
        "    {\n",
        "        for (int j = padder; j<width-padder;j++)\n",
        "        {\n",
        "            Sum = 0.0;\n",
        "            for (int k = (-1*filter_dim/2); k<=filter_dim/2;k++)\n",
        "            {\n",
        "                for (int l = (-1*filter_dim/2); l<=filter_dim/2;l++)\n",
        "                {\n",
        "                    val = src[(j+l) + (i+k)*width];\n",
        "                    fval = Filter[(l+filter_dim/2) + (k+filter_dim/2)*filter_dim];\n",
        "                    Sum += val*fval;\n",
        "                }\n",
        "            }\n",
        "         output[j + (i*width)] = Sum;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "// Convolution Global\n",
        "__global__ void naive_convolution(float* src, float* out, float* Filter, int width, int height,\n",
        "                                  int padder, int filter_dim)\n",
        "{\n",
        "    unsigned int col = threadIdx.x + blockIdx.x*blockDim.x;\n",
        "    unsigned int row = threadIdx.y + blockIdx.y*blockDim.y;\n",
        " \n",
        "    int row_start = row - padder ;\n",
        "    int col_start = col - padder;\n",
        " \n",
        "    float Sum = 0;\n",
        " \n",
        "    for (int i = 0; i < filter_dim;i++)\n",
        "    {\n",
        "        for (int j = 0; j < filter_dim;j++)\n",
        "        {\n",
        "            if((row_start + i) >= 0 && (row_start + i) < height)\n",
        "            {\n",
        "                if ((col_start + j) >=0 && (col_start + j) < width)\n",
        "                {\n",
        "                    Sum += src[(row_start + i)*width + (col_start + j)]*Filter[i*filter_dim + j];\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    out[row*width + col] = Sum;\n",
        " \n",
        " }\n",
        "\n",
        "\n",
        "// Convolution Constant Global\n",
        "__global__ void gConst_convolution(float* src, float* out, const float *__restrict__ kernel,\n",
        "                                   int width, int height,int padder, int filter_dim)\n",
        "{\n",
        "    unsigned int col = threadIdx.x + blockIdx.x*blockDim.x;\n",
        "    unsigned int row = threadIdx.y + blockIdx.y*blockDim.y;\n",
        " \n",
        "    int row_start = row - padder ;\n",
        "    int col_start = col - padder;\n",
        " \n",
        "    float Sum = 0;\n",
        " \n",
        "    for (int i = 0; i < filter_dim;i++)\n",
        "    {\n",
        "        for (int j = 0; j < filter_dim;j++)\n",
        "        {\n",
        "            if((row_start + i) >= 0 && (row_start + i) < height)\n",
        "            {\n",
        "                if ((col_start + j) >=0 && (col_start + j) < width)\n",
        "                {\n",
        "                    Sum += src[(row_start + i)*width + (col_start + j)]*deviceFilter[i*filter_dim + j];\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    out[row*width + col] = Sum;\n",
        " \n",
        " }\n",
        "\n",
        "////Share memory Convolution\n",
        "__global__ void share_convolution(float* src, float* out, float* Filter,\n",
        "                                   int width, int height,int padder, int filter_dim)\n",
        "{\n",
        "    __shared__ float s_array[block_size][block_size];\n",
        "\n",
        "    int tx = threadIdx.x;\n",
        "    int ty = threadIdx.y;\n",
        "\n",
        "    unsigned int col = tx + blockIdx.x*blockDim.x;\n",
        "    unsigned int row = ty + blockIdx.y*blockDim.y;\n",
        "    \n",
        "    const int t_loc = tx + blockDim.x*blockIdx.x + threadIdx.y*width + blockDim.y*blockIdx.y*width;\n",
        "\n",
        "    int row_start = row - padder ;\n",
        "    int col_start = col - padder;\n",
        "\n",
        "    if(row_start < 0 || col_start < 0){\n",
        "        s_array[ty][tx] = 0.0;\n",
        "    }else{\n",
        "        s_array[ty][tx] = src[t_loc - padder - width*padder];\n",
        "    }\n",
        "\n",
        "    row_start = row - padder ;\n",
        "    col_start = col + padder;\n",
        "    if(row_start < 0 || col_start > width - 1){\n",
        "         s_array[ty][tx + 2*padder] = 0.0;\n",
        "    }else{\n",
        "        s_array[ty][tx + 2*padder] = src[t_loc + padder - (width)*padder];\n",
        "    }\n",
        "\n",
        "    row_start = row + padder ;\n",
        "    col_start = col - padder;\n",
        "    if(row_start > height - 1 || col_start < 0){\n",
        "        s_array[ty+ 2*padder][tx] =  0.0;\n",
        "    }else{\n",
        "        s_array[ty+ 2*padder][tx] = src[t_loc - padder + (width)*padder];\n",
        "    }\n",
        "\n",
        "    row_start = row + padder;\n",
        "    col_start = col + padder;\n",
        "    if(row_start > height - 1 || col_start > width - 1){\n",
        "        s_array[ty+ 2*padder][tx+ 2*padder] = 0.0;\n",
        "    }else{\n",
        "        s_array[ty+ 2*padder][tx+ 2*padder] = src[t_loc + padder + (width)*padder];\n",
        "    }\n",
        " \n",
        "    __syncthreads();\n",
        " \n",
        "    row_start = ty + padder;\n",
        "    col_start = tx + padder;\n",
        " \n",
        "    float Sum = 0.0;\n",
        "    for (int y = -1*filter_dim/2; y < filter_dim/2 +1; y++)\n",
        "    {\n",
        "        for (int x = -1*filter_dim/2; x < filter_dim/2 +1;x++)\n",
        "        {\n",
        "            Sum += Filter[(y+padder)*filter_dim + (x + padder)]*s_array[y+row_start][x+col_start];\n",
        "        }\n",
        "    }\n",
        "    out[t_loc] = Sum;\n",
        "    \n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "//Shared Constant Memory Convolution\n",
        "__global__ void sConst_convolution(float* src, float* out, float *__restrict__ kernel,int width, int height,int padder, int filter_dim)\n",
        "{\n",
        "\n",
        "\n",
        "__shared__ float s_array[block_size][block_size];\n",
        "\n",
        "    int tx = threadIdx.x;\n",
        "    int ty = threadIdx.y;\n",
        "\n",
        "    unsigned int col = tx + blockIdx.x*blockDim.x;\n",
        "    unsigned int row = ty + blockIdx.y*blockDim.y;\n",
        "    \n",
        "    const int t_loc = tx + blockDim.x*blockIdx.x + threadIdx.y*width + blockDim.y*blockIdx.y*width;\n",
        "\n",
        "    int row_start = row - padder ;\n",
        "    int col_start = col - padder;\n",
        "\n",
        "    if(row_start < 0 || col_start < 0){\n",
        "        s_array[ty][tx] = 0.0;\n",
        "    }else{\n",
        "        s_array[ty][tx] = src[t_loc - padder - width*padder];\n",
        "    }\n",
        "\n",
        "    row_start = row - padder ;\n",
        "    col_start = col + padder;\n",
        "    if(row_start < 0 || col_start > width - 1){\n",
        "         s_array[ty][tx + 2*padder] = 0.0;\n",
        "    }else{\n",
        "        s_array[ty][tx + 2*padder] = src[t_loc + padder - (width)*padder];\n",
        "    }\n",
        "\n",
        "    row_start = row + padder ;\n",
        "    col_start = col - padder;\n",
        "    if(row_start > height - 1 || col_start < 0){\n",
        "        s_array[ty+ 2*padder][tx] =  0.0;\n",
        "    }else{\n",
        "        s_array[ty+ 2*padder][tx] = src[t_loc - padder + (width)*padder];\n",
        "    }\n",
        "\n",
        "    row_start = row + padder;\n",
        "    col_start = col + padder;\n",
        "    if(row_start > height - 1 || col_start > width - 1){\n",
        "        s_array[ty+ 2*padder][tx+ 2*padder] = 0.0;\n",
        "    }else{\n",
        "        s_array[ty+ 2*padder][tx+ 2*padder] = src[t_loc + padder + (width)*padder];\n",
        "    }\n",
        " \n",
        "    __syncthreads();\n",
        " \n",
        "    row_start = ty + padder;\n",
        "    col_start = tx + padder;\n",
        " \n",
        "    float Sum = 0.0;\n",
        "    for (int y = -1*filter_dim/2; y <= filter_dim/2; y++)\n",
        "    {\n",
        "        for (int x = -1*filter_dim/2; x <= filter_dim/2;x++)\n",
        "        {\n",
        "            Sum += deviceFilter[(y+padder)*filter_dim + (x + padder)]*s_array[y+row_start][x+col_start];\n",
        "        }\n",
        "    }\n",
        "    out[t_loc] = Sum;\n",
        "}\n",
        "\n",
        "\n",
        "// Texture Memory Convolution\n",
        "__global__ void tex_convolution( float* out, float* Filter, int width, int height,int filter_dim)\n",
        "{\n",
        "    unsigned int col = threadIdx.x + blockIdx.x*blockDim.x;\n",
        "    unsigned int row = threadIdx.y + blockIdx.y*blockDim.y;\n",
        " \n",
        "    float Sum = 0;\n",
        " \n",
        "    for (int i = -1*filter_dim/2; i < (filter_dim/2) +1;i++)\n",
        "    {\n",
        "        for (int j = -1*filter_dim/2; j < (filter_dim/2)+1;j++)\n",
        "        {\n",
        "            Sum += Filter[(i + filter_dim/2)*(filter_dim) + (j + filter_dim/2)]*tex2D(tex, col + j, row + i );\n",
        "        }\n",
        "         \n",
        "    }\n",
        "    out[row*width + col] = Sum;\n",
        " \n",
        " }\n",
        "\n",
        "\n",
        "int main()\n",
        "{\n",
        "    const char* imageFilename = str;\n",
        "    float* image = NULL;\n",
        "\tunsigned int width, height;\n",
        "    float* Filter = (float *) malloc(N*N*sizeof(float));\n",
        "    \n",
        "    sdkLoadPGM(\"/content/drive/My Drive/Colab Notebooks/HPC-Assignment-3/data/harvard1080.pgm\",&image,\n",
        "               &width, &height);\n",
        "\n",
        "    //Padding variables\n",
        "    unsigned int padder = N/2;\n",
        "    unsigned int padded_w = width+2*(padder);\n",
        "    unsigned int padded_h = height+2*(padder);\n",
        "    unsigned int pad_size = padded_h * padded_w * sizeof(float);\n",
        "    \n",
        "    //Host arrays\n",
        "    float* padded_image = (float *) malloc(pad_size);\n",
        "    float* output = (float *) malloc(pad_size);\n",
        "    \n",
        "    \n",
        "    //Device arrays\n",
        "    float* d_image;\n",
        "    float* d_filter;\n",
        "    float* d_output;\n",
        " \n",
        "\n",
        "    //Times\n",
        "    float time = 0;\n",
        "    float seq_time = 0;\n",
        "    float naive_time = 0;\n",
        "    float gConst_time = 0;\n",
        "    float share_time = 0;\n",
        "    float sConst_time = 0;\n",
        "    float tex_time = 0;\n",
        "    \n",
        "    \n",
        "    printf(\"============ %s %dx%d=============\\n\\n\", imageFilename, padded_w, padded_h);\n",
        "    printf(\"============ %s =======================\\n\\n\", filter_s);\n",
        "    printf(\"          Number of threads = %d \\n\\n\", tile_size);\n",
        "    \n",
        "    padding(image,width,height,padder,padded_image,padded_w,padded_h);\n",
        "    \n",
        "    //Initialise Timing(Sequential)\n",
        "    cudaEvent_t launch_begin, launch_end;\n",
        "    cudaEventCreate(&launch_begin);\n",
        "    cudaEventCreate(&launch_end);\n",
        "\n",
        "    for (int i = 0; i < Iter; i++)\n",
        "    {\n",
        "        memset(output,0,pad_size);\n",
        "        \n",
        "        cudaEventRecord(launch_begin,0);\n",
        "    \n",
        "        Seq_convolution(padded_image, filter,N,padded_w,padded_h,output,padder);\n",
        "    \n",
        "        cudaEventRecord(launch_end,0);\n",
        "        cudaEventSynchronize(launch_end);\n",
        "\n",
        "        cudaEventElapsedTime(&time, launch_begin, launch_end);\n",
        "\n",
        "        seq_time += time;\n",
        "\n",
        "    }\n",
        "    sdkSavePGM(\"/content/drive/My Drive/Colab Notebooks/HPC-Assignment-3/seq.pgm\", output,\n",
        "               padded_w,padded_h);\n",
        " \n",
        "    printf(\"Wrote Seq '%s', %d x %d pixels\\n\", \"seq.pgm\", padded_w, padded_h);\n",
        " \n",
        "    \n",
        "\n",
        "    printf(\"\\nCPU: Run Time: %f ms\\n\\n\", seq_time/Iter);\n",
        "///////////////////////////////////////CUDA////////////////////////////////////////////////////////////\n",
        "\n",
        "\n",
        "///////////////////////////////////////////Naive///////////////////////////////////////////////////////////////////\n",
        "    memset(output,0,pad_size);\n",
        "\n",
        "    //sdkLoadPGM(\"/content/drive/My Drive/Colab Notebooks/HPC-Assignment-3/data/lena_bw.pgm\",&image, &width, &height);\n",
        "    printf(\"Loaded Naive '%s', %d x %d pixels\\n\", imageFilename, width, height);\n",
        "\n",
        "    //Allocate Device Memory\n",
        "    cudaMalloc((void**)&d_image,pad_size);\n",
        "    cudaMalloc((void**)&d_output,pad_size);\n",
        "    cudaMalloc((void**)&d_filter,N*N*sizeof(float));\n",
        "\n",
        "    cudaMemcpy(d_image,padded_image, pad_size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_output,output, pad_size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_filter,filter, N*N*sizeof(float), cudaMemcpyHostToDevice);\n",
        "    \n",
        "\n",
        "    int threads = tile_size;\n",
        "    dim3 dimBlock(threads, threads);\n",
        "    dim3 dimGrid(ceil((float)padded_w/threads), ceil((float)padded_h/threads));\n",
        "    \n",
        "    for (int i = 0; i < Iter; i++)\n",
        "    {\n",
        "\n",
        "        cudaEventRecord(launch_begin,0);\n",
        "\n",
        "        naive_convolution<<<dimGrid,dimBlock>>>(d_image,d_output,d_filter,padded_w,padded_h,padder,N);\n",
        "        \n",
        "        cudaEventRecord(launch_end,0);\n",
        "        cudaEventSynchronize(launch_end);\n",
        "\n",
        "        cudaEventElapsedTime(&time, launch_begin, launch_end);\n",
        "\n",
        "        naive_time += time;\n",
        "    }\n",
        "    cudaMemcpy(output, d_output, pad_size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    checkCUDAError(\"Naive\");\n",
        "\n",
        "    sdkSavePGM(\"/content/drive/My Drive/Colab Notebooks/HPC-Assignment-3/Naive.pgm\", output, \n",
        "               padded_w,padded_h);\n",
        "    printf(\"Wrote Naive '%s', %d x %d pixels\\n\", \"Naive.pgm\",  padded_w, padded_h);\n",
        "    \n",
        "    printf(\"\\nGPU(Global): Run Time: %f ms\\n\\n\", naive_time/Iter);\n",
        "\n",
        "///////////////////////////////////////////Global Constant///////////////////////////////////////////////////////////////////\n",
        "    memset(output,0,pad_size);\n",
        "    cudaMemset(d_output, 0, pad_size);\n",
        "\n",
        "    //sdkLoadPGM(\"/content/drive/My Drive/Colab Notebooks/HPC-Assignment-3/data/lena_bw.pgm\",&image, &width, &height);\n",
        "    printf(\"Loaded GlobConst '%s', %d x %d pixels\\n\", imageFilename, width, height);\n",
        "\n",
        "\n",
        "    cudaMemcpy(d_image,padded_image, pad_size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_output,output, pad_size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpyToSymbol(deviceFilter, filter, N*N*sizeof(float));\n",
        "    \n",
        "    for (int i = 0; i < Iter; i++)\n",
        "    {\n",
        "\n",
        "        cudaEventRecord(launch_begin,0);\n",
        "\n",
        "        gConst_convolution<<<dimGrid,dimBlock>>>(d_image,d_output,d_filter,padded_w,padded_h,padder,N);\n",
        "        \n",
        "        cudaEventRecord(launch_end,0);\n",
        "        cudaEventSynchronize(launch_end);\n",
        "\n",
        "        cudaEventElapsedTime(&time, launch_begin, launch_end);\n",
        "\n",
        "        gConst_time += time;\n",
        "    }\n",
        "    cudaMemcpy(output, d_output, pad_size, cudaMemcpyDeviceToHost);\n",
        "    \n",
        "    checkCUDAError(\"gConst\");\n",
        "\n",
        "    sdkSavePGM(\"/content/drive/My Drive/Colab Notebooks/HPC-Assignment-3/gConst.pgm\", output, \n",
        "               padded_w,padded_h);\n",
        "    printf(\"Wrote GlobConst '%s', %d x %d pixels\\n\", \"gConst.pgm\",  padded_w, padded_h);\n",
        "\n",
        "    printf(\"\\nGPU(Global Constant): Run Time: %f ms\\n\\n\", gConst_time/Iter);\n",
        "\n",
        "///////////////////////////////Shared/////////////////////////////////////////////////////\n",
        "    memset(output,0,pad_size);\n",
        "    cudaMemset(d_output, 0, pad_size);\n",
        "\n",
        "    printf(\"Loaded share '%s', %d x %d pixels\\n\", imageFilename, width, height);\n",
        "\n",
        "\n",
        "    cudaMemcpy(d_image,padded_image, pad_size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_output,output, pad_size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_filter,filter, N*N*sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    \n",
        "    for (int i = 0; i < Iter; i++)\n",
        "    {\n",
        "        cudaEventRecord(launch_begin,0);\n",
        "\n",
        "        share_convolution<<<dimGrid,dimBlock>>>(d_image,d_output,d_filter,padded_w,padded_h,padder,N);\n",
        "        \n",
        "        cudaEventRecord(launch_end,0);\n",
        "        cudaEventSynchronize(launch_end);\n",
        "\n",
        "        cudaEventElapsedTime(&time, launch_begin, launch_end);\n",
        "        share_time += time;\n",
        "    }\n",
        "    cudaMemcpy(output, d_output, pad_size, cudaMemcpyDeviceToHost);\n",
        "    checkCUDAError(\"share\");\n",
        "\n",
        "    sdkSavePGM(\"/content/drive/My Drive/Colab Notebooks/HPC-Assignment-3/share.pgm\", output, \n",
        "                padded_w, padded_h);\n",
        "    printf(\"Wrote share '%s', %d x %d pixels\\n\", \"share.pgm\",  padded_w, padded_h);\n",
        "\n",
        "    printf(\"\\nGPU(Share) Tile Size = %d : Run Time: %f ms\\n\\n\",block_size, share_time/Iter);\n",
        "\n",
        "\n",
        "\n",
        "///////////////////////////////Shared Constant/////////////////////////////////////////////////////\n",
        "    memset(output,0,pad_size);\n",
        "    cudaMemset(d_output, 0, pad_size);\n",
        "\n",
        "    printf(\"Loaded share '%s', %d x %d pixels\\n\", imageFilename, width, height);\n",
        "\n",
        "\n",
        "    cudaMemcpy(d_image,padded_image, pad_size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_output,output, pad_size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpyToSymbol(deviceFilter, filter, N*N*sizeof(float));\n",
        "\n",
        "    for (int i = 0; i < Iter; i++)\n",
        "    {\n",
        "        cudaEventRecord(launch_begin,0);\n",
        "\n",
        "        sConst_convolution<<<dimGrid,dimBlock>>>(d_image,d_output,d_filter,padded_w,padded_h,padder,N);\n",
        "        \n",
        "        cudaEventRecord(launch_end,0);\n",
        "        cudaEventSynchronize(launch_end);\n",
        "\n",
        "        cudaEventElapsedTime(&time, launch_begin, launch_end);\n",
        "        sConst_time += time;\n",
        "    }\n",
        "    cudaMemcpy(output, d_output, pad_size, cudaMemcpyDeviceToHost);\n",
        "    checkCUDAError(\"sConst\");\n",
        "\n",
        "    sdkSavePGM(\"/content/drive/My Drive/Colab Notebooks/HPC-Assignment-3/sConst.pgm\", output, \n",
        "                padded_w, padded_h);\n",
        "    printf(\"Wrote share '%s', %d x %d pixels\\n\", \"sConst.pgm\",  padded_w, padded_h);\n",
        "\n",
        "    printf(\"GPU(Shared Constant) Tile Size = %d : Run Time: %f ms\\n\", block_size, sConst_time/Iter);\n",
        "\n",
        "\n",
        "\n",
        "///////////////////////////////Texture /////////////////////////////////////////////////////\n",
        "    cudaChannelFormatDesc channelDesc = cudaCreateChannelDesc(32,0,0,0, cudaChannelFormatKindFloat);\n",
        "    cudaArray* cuda_image;\n",
        "    checkCudaErrors(cudaMallocArray(&cuda_image, &channelDesc,padded_w,padded_h));\n",
        "    checkCudaErrors(cudaMemcpyToArray(cuda_image,0,0,padded_image, pad_size,cudaMemcpyHostToDevice));\n",
        " \n",
        "\n",
        "    tex.normalized = false;\n",
        "    tex.addressMode[0] = cudaAddressModeBorder;\n",
        "    tex.addressMode[1] = cudaAddressModeBorder;\n",
        "    tex.filterMode = cudaFilterModePoint;\n",
        " \n",
        "    checkCudaErrors(cudaBindTextureToArray(tex,cuda_image, channelDesc));\n",
        "    \n",
        "\n",
        "    memset(output,0,pad_size);\n",
        "    cudaMemset(d_output, 0, pad_size);\n",
        "\n",
        "    printf(\"Loaded tex '%s', %d x %d pixels\\n\", imageFilename, width, height);\n",
        "\n",
        "\n",
        "    cudaMemcpy(d_image,padded_image, pad_size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_output,output, pad_size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpyToSymbol(deviceFilter, filter, N*N*sizeof(float));\n",
        "\n",
        "    for (int i = 0; i < Iter; i++)\n",
        "    {\n",
        "        cudaEventRecord(launch_begin,0);\n",
        "\n",
        "        tex_convolution<<<dimGrid,dimBlock>>>(d_output,d_filter,padded_w,padded_h,N);\n",
        "    \n",
        "        cudaEventRecord(launch_end,0);\n",
        "        cudaEventSynchronize(launch_end);\n",
        "\n",
        "        cudaEventElapsedTime(&time, launch_begin, launch_end);\n",
        "        tex_time += time;\n",
        "    }\n",
        "\n",
        "    cudaMemcpy(output, d_output, pad_size, cudaMemcpyDeviceToHost);\n",
        "    checkCUDAError(\"tex\");\n",
        "\n",
        "    sdkSavePGM(\"/content/drive/My Drive/Colab Notebooks/HPC-Assignment-3/tex.pgm\", output, \n",
        "                padded_w, padded_h);\n",
        "    printf(\"Wrote share '%s', %d x %d pixels\\n\", \"tex.pgm\",  padded_w, padded_h);\n",
        "\n",
        "    printf(\"\\nGPU(Texture): Run Time: %f ms\\n\\n\", tex_time/Iter);\n",
        "\n",
        "\n",
        "\n",
        "    free(output);\n",
        "    free(padded_image);\n",
        "    cudaFree(d_image);\n",
        "    cudaFree(d_filter);\n",
        "    cudaFree(d_output);\n",
        "    cudaFreeArray(cuda_image);\n",
        "    cudaEventDestroy(launch_begin);\n",
        "    cudaEventDestroy(launch_end);\n",
        "    \n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY7LhfLKcsy4",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5RDS9RgZGzT",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmETYZQeWDrq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import numpy\n",
        "\n",
        "def read_pgm(filename, byteorder='>'):\n",
        "    \"\"\"Return image data from a raw PGM file as numpy array.\n",
        "\n",
        "    Format specification: http://netpbm.sourceforge.net/doc/pgm.html\n",
        "\n",
        "    \"\"\"\n",
        "    with open(filename, 'rb') as f:\n",
        "        buffer = f.read()\n",
        "    try:\n",
        "        header, width, height, maxval = re.search(\n",
        "            b\"(^P5\\s(?:\\s*#.*[\\r\\n])*\"\n",
        "            b\"(\\d+)\\s(?:\\s*#.*[\\r\\n])*\"\n",
        "            b\"(\\d+)\\s(?:\\s*#.*[\\r\\n])*\"\n",
        "            b\"(\\d+)\\s(?:\\s*#.*[\\r\\n]\\s)*)\", buffer).groups()\n",
        "    except AttributeError:\n",
        "        raise ValueError(\"Not a raw PGM file: '%s'\" % filename)\n",
        "    return numpy.frombuffer(buffer,\n",
        "                            dtype='u1' if int(maxval) < 256 else byteorder+'u2',\n",
        "                            count=int(width)*int(height),\n",
        "                            offset=len(header)\n",
        "                            ).reshape((int(height), int(width)))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    from matplotlib import pyplot\n",
        "    image = read_pgm(\"/content/drive/My Drive/Colab Notebooks/HPC-Assignment-3/data/image21.pgm\", byteorder='<')\n",
        "    pyplot.imshow(image, pyplot.cm.gray)\n",
        "    pyplot.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    from matplotlib import pyplot\n",
        "    image = read_pgm(\"/content/drive/My Drive/Colab Notebooks/HPC-Assignment-3/seq.pgm\", byteorder='<')\n",
        "    pyplot.imshow(image, pyplot.cm.gray)\n",
        "    pyplot.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    from matplotlib import pyplot\n",
        "    image = read_pgm(\"/content/drive/My Drive/Colab Notebooks/HPC-Assignment-3/Naive.pgm\", byteorder='<')\n",
        "    pyplot.imshow(image, pyplot.cm.gray)\n",
        "    pyplot.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    from matplotlib import pyplot\n",
        "    image = read_pgm(\"/content/drive/My Drive/Colab Notebooks/HPC-Assignment-3/gConst.pgm\", byteorder='<')\n",
        "    pyplot.imshow(image, pyplot.cm.gray)\n",
        "    pyplot.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    from matplotlib import pyplot\n",
        "    image = read_pgm(\"/content/drive/My Drive/Colab Notebooks/HPC-Assignment-3/share.pgm\", byteorder='<')\n",
        "    pyplot.imshow(image, pyplot.cm.gray)\n",
        "    pyplot.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    from matplotlib import pyplot\n",
        "    image = read_pgm(\"/content/drive/My Drive/Colab Notebooks/HPC-Assignment-3/sConst.pgm\", byteorder='<')\n",
        "    pyplot.imshow(image, pyplot.cm.gray)\n",
        "    pyplot.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    from matplotlib import pyplot\n",
        "    image = read_pgm(\"/content/drive/My Drive/Colab Notebooks/HPC-Assignment-3/tex.pgm\", byteorder='<')\n",
        "    pyplot.imshow(image, pyplot.cm.gray)\n",
        "    pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zitwhhmRp50",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4807f34c-9768-4b87-dcea-2b395c435579"
      },
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks/HPC-Assignment3/src\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/HPC-Assignment3/src\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGnnybwBtITW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "84eb975d-e6c8-417c-9329-4290d88160ea"
      },
      "source": [
        "!make"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "make: Nothing to be done for 'all'.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRN727g33fzV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "5068fc46-1b8b-49dd-adb1-7d87a9e397dd"
      },
      "source": [
        "!./convolv"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============ ./data/harvard1080.pgm 1082x1082=============\n",
            "\n",
            "============ Blur 3x3 =======================\n",
            "\n",
            "          Number of threads = 16 \n",
            "\n",
            "CPU:Run Time:60.082275 ms\n",
            "GPU(Global):Run Time:0.528995 ms\n",
            "GPU(Global Constant):Run Time:0.251389 ms\n",
            "GPU(Share) Tile Size = 18:Run Time:0.457114 ms\n",
            "GPU(Shared Constant) Tile Size = 18:Run Time:0.153293 ms\n",
            "GPU(Texture):Run Time:0.468480 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1p6MZZo14if4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}